{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "pediatric-island",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib3\n",
    "import json\n",
    "import re\n",
    "import sys\n",
    "from time import time\n",
    "import signal\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imposed-victorian",
   "metadata": {},
   "source": [
    "## Read/download a website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "defined-virtue",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_site(url, pool=None, decode=True):\n",
    "    \"\"\"\n",
    "    Fetching the contents from url.\n",
    "    args:\n",
    "        url (str): The url!\n",
    "        pool (urllib3.PoolManager): The pool that we use to fetch the url\n",
    "            If it is \"None\", it will create a pool.\n",
    "        decode (bool): If False, return the raw website (object).\n",
    "                       If True, return the decoded page\n",
    "    return:\n",
    "        site content and a dict of extra information (elapsed time)\n",
    "    \"\"\"\n",
    "    if pool is None:\n",
    "        pool = urllib3.PoolManager()\n",
    "    t1 = time()\n",
    "    site = pool.request('GET', url)\n",
    "    elapsed_time = time() - t1\n",
    "    if decode:\n",
    "        site = site.data.decode(\"utf-8\")\n",
    "    return site, {'fetching time (s)': elapsed_time}\n",
    "\n",
    "\n",
    "def test_download_site():\n",
    "    pool = urllib3.PoolManager()\n",
    "    url = 'http://www.brainjar.com/java/host/test.html'\n",
    "    content, info = download_site(url, pool, decode=False)\n",
    "    assert content.status == 200\n",
    "    assert len(content.data) > 0\n",
    "\n",
    "test_download_site()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "accessible-metallic",
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_site(url, regex_list, pool=None, unique=False):\n",
    "    \"\"\"\n",
    "    Read an url and return matched patterns\n",
    "    args:\n",
    "        url (str): The url\n",
    "        regex_list (list): a list contains regex strings\n",
    "        pool (urllib3.PoolManager): The pool that we use to fetch the url\n",
    "            If it is \"None\", it will create a pool.\n",
    "        unique (bool): If true, it remove the matched redundancies\n",
    "    return:\n",
    "        a dict in which the keys are the regex strings and values are\n",
    "            a list of matched values from the url.\n",
    "        another dict of extra information\n",
    "    \"\"\"\n",
    "    content, info = download_site(url, pool=pool, decode=True)\n",
    "    matched = {}\n",
    "    t1 = time()\n",
    "    for regex in regex_list:\n",
    "        matched[regex] = re.findall(regex, content)\n",
    "    processing_time = time() - t1\n",
    "    info['processing time (s)'] = processing_time\n",
    "    return matched, info\n",
    "\n",
    "def test_match_site():\n",
    "    pool = urllib3.PoolManager()\n",
    "    url = 'https://webscraper.io/test-sites/tables'\n",
    "    matched, info = match_site(url, [r'@\\w{10,}', r'\\d{6,}'])\n",
    "    assert (matched == {'@\\\\w{10,}': ['@webscraper', '@webscraper'], '\\\\d{6,}': ['604046']})\n",
    "\n",
    "test_match_site()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "ethical-passion",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intrupt by user!\n",
      "Intrupt by user!\n"
     ]
    }
   ],
   "source": [
    "def create_test_json(test_file):\n",
    "    \"\"\"\n",
    "    Creating a sample json file for testing\n",
    "    args:\n",
    "        test_file (str): name of \n",
    "    \"\"\"\n",
    "    data = [{'https://webscraper.io/test-sites/tables': ['\\\\d{4,}', '@\\\\w+']}] * 100\n",
    "    json.dump(data, open(test_file, 'w+'), indent=2)\n",
    "\n",
    "    \n",
    "def download_all(input_file, output_file, unique=False):\n",
    "    global all_matched\n",
    "    all_matched = []\n",
    "    def signal_handler(sig, frame):\n",
    "        print('Intrupt by user!')\n",
    "        json.dump(all_matched, open(output_file, 'w+'), indent=2)\n",
    "\n",
    "    signal.signal(signal.SIGINT, signal_handler)\n",
    "\n",
    "    pool = urllib3.PoolManager()    \n",
    "    for sample in json.load(open(input_file, 'r')):\n",
    "        url, regex_list = list(sample.items())[0]\n",
    "        matched, info = match_site(url, regex_list, pool=pool, unique=unique)\n",
    "        matched = {url: matched}\n",
    "        for key in info:\n",
    "            matched[url][key] = info[key]\n",
    "        all_matched.append(matched)\n",
    "#     print(all_matched)\n",
    "    json.dump(all_matched, open(output_file, 'w+'), indent=2)\n",
    "    \n",
    "    \n",
    "\n",
    "create_test_json('test.json')\n",
    "download_all('test.json', 'res.json')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
